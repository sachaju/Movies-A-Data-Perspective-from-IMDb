{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4982b936",
   "metadata": {},
   "source": [
    "# Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab71536a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for web scraping and data handling\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca38bbd",
   "metadata": {},
   "source": [
    "# SCRAPING STEP 1\n",
    "# Collect all the movie links from the IMDb page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c33c1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Chrome driver options\n",
    "options = Options()\n",
    "options.add_experimental_option('prefs', {'intl.accept_languages': 'en'})  # Ensure English language preference\n",
    "# options.add_argument('--headless')  # Uncomment to enable headless mode (no browser window)\n",
    "options.add_argument(\"--disable-search-engine-choice-screen\")  # Disable search engine selection prompts\n",
    "\n",
    "# IMDb URL for the movies sorted by number of votes\n",
    "url = \"https://www.imdb.com/search/title/?title_type=feature&count=100&sort=num_votes,desc\"\n",
    "\n",
    "# Launch the Chrome WebDriver\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.get(url)\n",
    "\n",
    "# Close the cookie consent pop-up (if present)\n",
    "cookies_button = WebDriverWait(driver, 10).until(\n",
    "    EC.element_to_be_clickable((By.XPATH, '/html/body/div[2]/div/div/div[2]/div/button[1]'))\n",
    ")\n",
    "cookies_button.click()\n",
    "\n",
    "# Function to scroll the page and click \"Load More\" buttons (if available)\n",
    "def scroll_and_click_button():\n",
    "    \"\"\"\n",
    "    Scrolls through the IMDb page and clicks the \"Load More\" button to reveal additional results.\n",
    "    \"\"\"\n",
    "    # Wait for the page's main content to load\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "\n",
    "    # XPath of the \"Load More\" button\n",
    "    button_xpath = '//*[@id=\"__next\"]/main/div[2]/div[3]/section/section/div/section/section/div[2]/div/section/div[2]/div[2]/div[2]/div/span/button/span/span'\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Attempt to locate and click the button\n",
    "            button = WebDriverWait(driver, 2).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, button_xpath))\n",
    "            )\n",
    "            button.click()\n",
    "            print(\"Button found and clicked.\")\n",
    "            break  # Exit the loop once the button is successfully clicked\n",
    "        except Exception:\n",
    "            # Scroll down if the button isn't found\n",
    "            driver.execute_script(\"window.scrollBy(0, 300);\")\n",
    "            time.sleep(1)  # Pause briefly to prevent overloading the browser\n",
    "\n",
    "# Execute the scroll-and-click function multiple times\n",
    "for _ in range(1):\n",
    "    scroll_and_click_button()\n",
    "\n",
    "# Function to extract movie links from the currently loaded page\n",
    "def extract_links():\n",
    "    \"\"\"\n",
    "    Extracts all movie links from the IMDb search results page.\n",
    "    \"\"\"\n",
    "    # Save the page's HTML content for debugging purposes\n",
    "    html_content = driver.page_source\n",
    "    with open('page_source.html', 'w', encoding='utf-8') as f:\n",
    "        f.write(html_content)\n",
    "    print(\"HTML of the page saved as 'page_source.html' for diagnostics.\")\n",
    "\n",
    "    links = []\n",
    "\n",
    "    # Locate all <a> tags containing <h3> elements, which represent movie links\n",
    "    results = driver.find_elements(By.XPATH, '//*[contains(@id, \"__next\")]//a/h3')\n",
    "    if not results:\n",
    "        print(\"No links found with the current selector.\")\n",
    "    else:\n",
    "        print(f\"Found {len(results)} links.\")\n",
    "\n",
    "    # Extract the href attribute from the parent <a> tag of each <h3>\n",
    "    for result in results:\n",
    "        link = result.find_element(By.XPATH, '..').get_attribute('href')\n",
    "        if link:\n",
    "            links.append(link)\n",
    "\n",
    "    return links\n",
    "\n",
    "# Extract all movie links from the page\n",
    "all_links = extract_links()\n",
    "\n",
    "# Save the extracted links to a text file\n",
    "with open('imdb_links.txt', 'w', encoding='utf-8') as file:\n",
    "    for link in all_links:\n",
    "        if link:\n",
    "            file.write(link + '\\n')\n",
    "\n",
    "# Print the total number of links extracted\n",
    "print(f\"Total links extracted: {len(all_links)}\")\n",
    "\n",
    "# Close the browser once the links are collected\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b0c491",
   "metadata": {},
   "source": [
    "# SCRAPING STEP 2\n",
    "# Collect detailed movie information from each link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9665d4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCRAPING STEP 2\n",
    "# Collect detailed movie information from each link\n",
    "\n",
    "# Function to get details about a movie\n",
    "def get_movie_details(driver, movie_url):\n",
    "    driver.get(movie_url)\n",
    "    time.sleep(3)  # Wait for the page to load\n",
    "    \n",
    "    # Parse the page source with BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    movie_data = {\n",
    "        \"Title\": \"N/A\",\n",
    "        \"Rating\": \"N/A\",\n",
    "        \"Directors\": \"N/A\",\n",
    "        \"Writers\": \"N/A\",\n",
    "        \"Actors\": \"N/A\",\n",
    "        \"Production Company\": \"N/A\",\n",
    "        \"Release Date\": \"N/A\",\n",
    "        \"Genres\": \"N/A\",\n",
    "        \"Parental Guide\": \"N/A\",\n",
    "        \"Country of Origin\": \"N/A\",\n",
    "        \"Languages\": \"N/A\",\n",
    "        \"Runtime\": \"N/A\",\n",
    "        \"Box Office\": \"N/A\",\n",
    "        \"Budget\": \"N/A\",\n",
    "        \"Plot Summary\": \"N/A\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Extract movie details\n",
    "        movie_data['Title'] = soup.find('h1').text.strip() if soup.find('h1') else 'N/A'\n",
    "        \n",
    "        # Rating\n",
    "        movie_data['Rating'] = soup.find('span', class_='sc-d541859f-1 imUuxf').text.strip() if soup.find('span', class_='sc-d541859f-1 imUuxf') else 'N/A'\n",
    "        \n",
    "        # Directors\n",
    "        directors_elements = soup.find_all('a', href=lambda x: x and 'tt_ov_dr_' in x)\n",
    "        movie_data['Directors'] = ', '.join(set(director.text.strip() for director in directors_elements)) if directors_elements else 'N/A'\n",
    "        \n",
    "        # Writers\n",
    "        writers_elements = soup.find_all('a', href=lambda x: x and 'tt_ov_wr_' in x)\n",
    "        movie_data['Writers'] = ', '.join(set(writer.text.strip() for writer in writers_elements)) if writers_elements else 'N/A'\n",
    "        \n",
    "        # Actors\n",
    "        actors_elements = soup.find_all('a', class_=\"sc-cd7dc4b7-1 kVdWAO\")\n",
    "        movie_data['Actors'] = ', '.join(actor.text.strip() for actor in actors_elements) if actors_elements else 'N/A'\n",
    "        \n",
    "        # Production Company\n",
    "        produc_elements = soup.find_all('a', href=lambda x: x and 'tt_dt_cmpy_' in x)\n",
    "        movie_data['Production Company'] = ', '.join(set(produc.text.strip() for produc in produc_elements)) if produc_elements else 'N/A'\n",
    "        \n",
    "        # Release Date\n",
    "        release_date = soup.select_one('a.ipc-link[href*=\"releaseinfo\"]')\n",
    "        movie_data['Release Date'] = release_date.text.strip() if release_date else 'N/A'\n",
    "        \n",
    "        # Genres\n",
    "        genre_tag = soup.find_all('span', class_='ipc-chip__text')\n",
    "        all_genres = [genre.get_text(strip=True) for genre in genre_tag if genre.get_text(strip=True) != 'Back to top']\n",
    "        movie_data['Genres'] = ', '.join(all_genres) if all_genres else 'N/A'\n",
    "\n",
    "        # Parental guide\n",
    "        guide_tag = soup.find('a', href=lambda x: x and 'parentalguide' in x)\n",
    "        movie_data['Parental Guide'] = guide_tag.text if guide_tag else 'N/A'\n",
    "        \n",
    "        # Country of origin\n",
    "        country_list = soup.find_all('a', class_='ipc-metadata-list-item__list-content-item ipc-metadata-list-item__list-content-item--link')\n",
    "        movie_data['Country of Origin'] = ', '.join([country.text.strip() for country in country_list if 'country_of_origin' in country['href'].lower()]) if country_list else 'N/A'\n",
    "        \n",
    "        # Languages\n",
    "        language_list = soup.find_all('a', class_='ipc-metadata-list-item__list-content-item ipc-metadata-list-item__list-content-item--link')\n",
    "        movie_data['Languages'] = ', '.join([language.text.strip() for language in language_list if 'primary_language' in language['href'].lower()]) if language_list else 'N/A'\n",
    "        \n",
    "       # Extract the duration\n",
    "        ul_tag = soup.find('ul', class_='ipc-inline-list ipc-inline-list--show-dividers sc-ec65ba05-2 joVhBE baseAlt')\n",
    "        duration_tag = ul_tag.find_all('li')[-1] if ul_tag else None\n",
    "        movie_data['Runtime'] = duration_tag.text.strip() if duration_tag else 'N/A'\n",
    "        \n",
    "        # Box Office\n",
    "        box_office_tag = soup.find('li', {'data-testid': 'title-boxoffice-cumulativeworldwidegross'}).find('span', class_='ipc-metadata-list-item__list-content-item')\n",
    "        movie_data['Box Office'] = box_office_tag.text.strip() if box_office_tag else 'N/A'\n",
    "        \n",
    "        # Budget\n",
    "        budget_tag = soup.find('li', {'data-testid': 'title-boxoffice-budget'}).find('span', class_='ipc-metadata-list-item__list-content-item')\n",
    "        movie_data['Budget'] = budget_tag.text.strip() if budget_tag else 'N/A'\n",
    "        \n",
    "        # Plot Summary\n",
    "        plot_summary = soup.find('span', class_='sc-3ac15c8d-0 hRUoSB')\n",
    "        movie_data['Plot Summary'] = plot_summary.text.strip() if plot_summary else 'N/A'\n",
    "        \n",
    "        return movie_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error collecting data for '{movie_url}': {e}\")\n",
    "        return None\n",
    "\n",
    "# Set up Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_experimental_option('prefs', {'intl.accept_languages': 'en'})\n",
    "chrome_options.add_argument(\"--disable-search-engine-choice-screen\")\n",
    "chrome_options.add_argument(\"--start-maximized\")  # Maximize the window for convenience\n",
    "\n",
    "# Set up the WebDriver\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "# List to store the scraped movie data\n",
    "movies_data = []\n",
    "\n",
    "# Loop through each movie link and scrape details\n",
    "for movie_url in all_links:\n",
    "    movie_details = get_movie_details(driver, movie_url)\n",
    "    if movie_details:\n",
    "        movies_data.append(movie_details)\n",
    "        print(f\"Scraped: {movie_details['Title']}\")  # Track progress\n",
    "\n",
    "# Save the movie data to a CSV file\n",
    "csv_file = 'movies_data_1.csv'\n",
    "csv_columns = ['Title', 'Rating','Directors','Release Date','Writers','Actors','Production Company','Plot Summary', 'Genres', 'Parental Guide', 'Country of Origin', 'Languages', 'Runtime', 'Box Office','Budget']\n",
    "\n",
    "with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=csv_columns)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(movies_data)\n",
    "\n",
    "# Close the Selenium WebDriver\n",
    "driver.quit()\n",
    "\n",
    "print(f\"Scraped data for {len(movies_data)} movies and saved to {csv_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93c842a",
   "metadata": {},
   "source": [
    "# SCRAPING STEP 3\n",
    "# Extract and save all image links for movies to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6feb995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCRAPING STEP 3\n",
    "# Scrape all image links for movies and save them to a CSV file\n",
    "\n",
    "# Set up for Selenium and ChromeDriver\n",
    "options = Options()\n",
    "options.add_experimental_option('prefs', {'intl.accept_languages': 'en'})\n",
    "# options.add_argument('--headless')  # Esegui in modalità headless (senza interfaccia grafica)\n",
    "options.add_argument(\"--disable-search-engine-choice-screen\")\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Open a CSV to write data\n",
    "with open('movie_images_1.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['Title', 'Image Link']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()  \n",
    "\n",
    "    for url in all_links:\n",
    "        driver.get(url)\n",
    "\n",
    "        # Close cookies\n",
    "        try:\n",
    "            cookies_button = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, '/html/body/div[2]/div/div/div[2]/div/button[1]'))\n",
    "            )\n",
    "            cookies_button.click()\n",
    "        except Exception as e:\n",
    "            print(f\"Non è stato possibile chiudere i cookies per {url}. Errore: {e}\")\n",
    "\n",
    "        try:\n",
    "            # Find title\n",
    "            movie_title_element = driver.find_element(By.XPATH, '//*[@id=\"__next\"]/main/div/section[1]/section/div[3]/section/section/div[2]/div[1]/h1/span')\n",
    "            movie_title = movie_title_element.text.strip()\n",
    "\n",
    "            # Find button to open the img\n",
    "            enlarge_button = driver.find_element(By.XPATH, '//*[@id=\"__next\"]/main/div/section[1]/section/div[3]/section/section/div[3]/div[1]/div[1]/div/a')\n",
    "            \n",
    "            # Click on button to open img\n",
    "            enlarge_button.click()\n",
    "\n",
    "            # Wait\n",
    "            time.sleep(3)\n",
    "\n",
    "            # Find the img\n",
    "            enlarged_img_element = driver.find_element(By.XPATH, '/html/body/div[2]/main/div[2]/div[3]/div[4]/img')\n",
    "            img_url = enlarged_img_element.get_attribute(\"src\")\n",
    "\n",
    "            # Write title and url to CSV\n",
    "            writer.writerow({'Title': movie_title, 'Image Link': img_url})\n",
    "            print(f\"Titolo: {movie_title} - Link dell'immagine: {img_url}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Errore durante l'elaborazione di {url}: {e}\")\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe9a00b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
